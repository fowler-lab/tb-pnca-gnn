{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running via SSH'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "if \"SSH_CONNECTION\" in os.environ:\n",
    "    display(\"Running via SSH\")\n",
    "else:\n",
    "    display(\"Running locally\")\n",
    "    \n",
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.join('..', '/Users/dylandissanayake/Desktop/DPhil/Comp Disc/Repositories/TB-PNCA-GNN') if \"SSH_CONNECTION\" not in os.environ else os.path.join('..', '/mnt/alphafold-volume-1/dylan2/repos/tb-pnca-gnn')\n",
    "if path not in sys.path:\n",
    "    sys.path.append(os.path.abspath(path))\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src import run_model, protein_graph, gcn_model, evaluation\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%aimport src\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/x18_exp_l2_graph_dict.pkl', 'rb') as f:\n",
    "    graph_dict = pkl.load(f)\n",
    "    \n",
    "print(len(graph_dict['train']) + len(graph_dict['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Best param value ranges from WandB sweeps\n",
    "\n",
    "- Cutoff distance = 11 - 12.5\n",
    "- Dropout = 0.4-0.5\n",
    "- Edge weights = \"exp\"\n",
    "- Edge weight lambda = 2\n",
    "- Hidden channels = 320\n",
    "- Learning rate = 3.5e-5 - 4.5e-5\n",
    "- Weight decay = 1e-6 - 1e-5\n",
    "\n",
    "New best params for 18 feature model\n",
    "\n",
    "- Dropout = 0.6\n",
    "- Hidden channels = 256\n",
    "- Learning rate = 4e-5\n",
    "- Weight decay = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# logging params (only used for wandb metrics)\n",
    "n_samples = len(graph_dict['train']) + len(graph_dict['test'])\n",
    "cutoff_distance = 12\n",
    "\n",
    "# gcn params - from best wandb sweep\n",
    "# num_node_features = 18\n",
    "num_node_features = 14\n",
    "batch_size = 256\n",
    "# hidden_channels = 240\n",
    "hidden_channels = 320\n",
    "dropout = 0.6\n",
    "\n",
    "edge_weight_func = \"exp\"\n",
    "edge_weight_lambda = 2\n",
    "\n",
    "learning_rate = 4e-5\n",
    "wd = 5e-6\n",
    "epochs = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'train-final-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove edge weights\n",
    "# for split in graph_dict:\n",
    "#     for sample in graph_dict[split]:\n",
    "#         graph_dict[split][sample]['graph'].dataset[0]['edge_attr'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[185, 18], edge_index=[2, 5452], edge_attr=[5452, 1], y=0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_dict['train']['pnca_mut_0']['graph'].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting edge index and attaching edge weights for cutoff distance 12\n",
      "Removing metapredictor features from node features\n",
      "Using CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdylan-home\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/alphafold-volume-1/dylan2/repos/tb-pnca-gnn/dd_pnca/wandb/run-20250714_103342-eh4pnwna</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dylan-home/train-final-model/runs/eh4pnwna' target=\"_blank\">Only_SBMLCore_9-2500epochs</a></strong> to <a href='https://wandb.ai/dylan-home/train-final-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dylan-home/train-final-model' target=\"_blank\">https://wandb.ai/dylan-home/train-final-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dylan-home/train-final-model/runs/eh4pnwna' target=\"_blank\">https://wandb.ai/dylan-home/train-final-model/runs/eh4pnwna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train Acc: 0.5302, Test Acc: 0.5150, Train Loss: 0.6923, Test Loss: 0.6929\n",
      "Epoch: 010, Train Acc: 0.5302, Test Acc: 0.5150, Train Loss: 0.6917, Test Loss: 0.6925\n",
      "Epoch: 020, Train Acc: 0.5259, Test Acc: 0.5150, Train Loss: 0.6907, Test Loss: 0.6920\n",
      "Epoch: 030, Train Acc: 0.5366, Test Acc: 0.5100, Train Loss: 0.6899, Test Loss: 0.6913\n",
      "Epoch: 040, Train Acc: 0.5366, Test Acc: 0.5150, Train Loss: 0.6895, Test Loss: 0.6914\n",
      "Epoch: 050, Train Acc: 0.5345, Test Acc: 0.5150, Train Loss: 0.6900, Test Loss: 0.6913\n",
      "Epoch: 060, Train Acc: 0.5259, Test Acc: 0.5150, Train Loss: 0.6895, Test Loss: 0.6915\n",
      "Epoch: 070, Train Acc: 0.5431, Test Acc: 0.5200, Train Loss: 0.6879, Test Loss: 0.6908\n",
      "Epoch: 080, Train Acc: 0.5280, Test Acc: 0.5150, Train Loss: 0.6890, Test Loss: 0.6912\n",
      "Epoch: 090, Train Acc: 0.5431, Test Acc: 0.4850, Train Loss: 0.6881, Test Loss: 0.6901\n",
      "Epoch: 100, Train Acc: 0.5388, Test Acc: 0.5200, Train Loss: 0.6885, Test Loss: 0.6904\n",
      "Epoch: 110, Train Acc: 0.5302, Test Acc: 0.5150, Train Loss: 0.6875, Test Loss: 0.6905\n",
      "Epoch: 120, Train Acc: 0.5517, Test Acc: 0.5100, Train Loss: 0.6873, Test Loss: 0.6899\n",
      "Epoch: 130, Train Acc: 0.5560, Test Acc: 0.5150, Train Loss: 0.6865, Test Loss: 0.6899\n",
      "Epoch: 140, Train Acc: 0.5496, Test Acc: 0.5050, Train Loss: 0.6871, Test Loss: 0.6894\n",
      "Epoch: 150, Train Acc: 0.5453, Test Acc: 0.5150, Train Loss: 0.6874, Test Loss: 0.6896\n",
      "Epoch: 160, Train Acc: 0.5496, Test Acc: 0.5150, Train Loss: 0.6862, Test Loss: 0.6893\n",
      "Epoch: 170, Train Acc: 0.5453, Test Acc: 0.5100, Train Loss: 0.6864, Test Loss: 0.6891\n",
      "Epoch: 180, Train Acc: 0.5409, Test Acc: 0.5150, Train Loss: 0.6861, Test Loss: 0.6889\n",
      "Epoch: 190, Train Acc: 0.5388, Test Acc: 0.5100, Train Loss: 0.6863, Test Loss: 0.6893\n",
      "Epoch: 200, Train Acc: 0.5453, Test Acc: 0.5150, Train Loss: 0.6853, Test Loss: 0.6887\n",
      "Epoch: 210, Train Acc: 0.5366, Test Acc: 0.5150, Train Loss: 0.6861, Test Loss: 0.6892\n",
      "Epoch: 220, Train Acc: 0.5496, Test Acc: 0.5200, Train Loss: 0.6851, Test Loss: 0.6884\n",
      "Epoch: 230, Train Acc: 0.5474, Test Acc: 0.5150, Train Loss: 0.6845, Test Loss: 0.6881\n",
      "Epoch: 240, Train Acc: 0.5453, Test Acc: 0.5150, Train Loss: 0.6838, Test Loss: 0.6880\n",
      "Epoch: 250, Train Acc: 0.5603, Test Acc: 0.5150, Train Loss: 0.6838, Test Loss: 0.6880\n",
      "Epoch: 260, Train Acc: 0.5409, Test Acc: 0.5250, Train Loss: 0.6837, Test Loss: 0.6878\n",
      "Epoch: 270, Train Acc: 0.5517, Test Acc: 0.5250, Train Loss: 0.6837, Test Loss: 0.6877\n",
      "Epoch: 280, Train Acc: 0.5431, Test Acc: 0.5200, Train Loss: 0.6828, Test Loss: 0.6873\n",
      "Epoch: 290, Train Acc: 0.5560, Test Acc: 0.5250, Train Loss: 0.6815, Test Loss: 0.6871\n",
      "Epoch: 300, Train Acc: 0.5409, Test Acc: 0.5200, Train Loss: 0.6817, Test Loss: 0.6868\n",
      "Epoch: 310, Train Acc: 0.5431, Test Acc: 0.5200, Train Loss: 0.6818, Test Loss: 0.6866\n",
      "Epoch: 320, Train Acc: 0.5517, Test Acc: 0.5150, Train Loss: 0.6810, Test Loss: 0.6861\n",
      "Epoch: 330, Train Acc: 0.5647, Test Acc: 0.5200, Train Loss: 0.6795, Test Loss: 0.6857\n",
      "Epoch: 340, Train Acc: 0.5603, Test Acc: 0.5150, Train Loss: 0.6799, Test Loss: 0.6855\n",
      "Epoch: 350, Train Acc: 0.5582, Test Acc: 0.5250, Train Loss: 0.6786, Test Loss: 0.6854\n",
      "Epoch: 360, Train Acc: 0.5582, Test Acc: 0.5200, Train Loss: 0.6787, Test Loss: 0.6854\n",
      "Epoch: 370, Train Acc: 0.5668, Test Acc: 0.5350, Train Loss: 0.6782, Test Loss: 0.6850\n",
      "Epoch: 380, Train Acc: 0.5819, Test Acc: 0.5300, Train Loss: 0.6775, Test Loss: 0.6843\n",
      "Epoch: 390, Train Acc: 0.5776, Test Acc: 0.5300, Train Loss: 0.6780, Test Loss: 0.6839\n",
      "Epoch: 400, Train Acc: 0.5948, Test Acc: 0.5400, Train Loss: 0.6764, Test Loss: 0.6837\n",
      "Epoch: 410, Train Acc: 0.5668, Test Acc: 0.5150, Train Loss: 0.6753, Test Loss: 0.6838\n",
      "Epoch: 420, Train Acc: 0.5690, Test Acc: 0.5200, Train Loss: 0.6742, Test Loss: 0.6836\n",
      "Epoch: 430, Train Acc: 0.5797, Test Acc: 0.5150, Train Loss: 0.6739, Test Loss: 0.6832\n",
      "Epoch: 440, Train Acc: 0.6013, Test Acc: 0.5400, Train Loss: 0.6727, Test Loss: 0.6827\n",
      "Epoch: 450, Train Acc: 0.6013, Test Acc: 0.5400, Train Loss: 0.6722, Test Loss: 0.6822\n",
      "Epoch: 460, Train Acc: 0.6078, Test Acc: 0.5600, Train Loss: 0.6725, Test Loss: 0.6817\n",
      "Epoch: 470, Train Acc: 0.5991, Test Acc: 0.5550, Train Loss: 0.6704, Test Loss: 0.6813\n",
      "Epoch: 480, Train Acc: 0.6099, Test Acc: 0.5350, Train Loss: 0.6693, Test Loss: 0.6810\n",
      "Epoch: 490, Train Acc: 0.6099, Test Acc: 0.5550, Train Loss: 0.6690, Test Loss: 0.6807\n",
      "Epoch: 500, Train Acc: 0.6272, Test Acc: 0.5700, Train Loss: 0.6682, Test Loss: 0.6803\n",
      "Epoch: 510, Train Acc: 0.6250, Test Acc: 0.5650, Train Loss: 0.6677, Test Loss: 0.6799\n",
      "Epoch: 520, Train Acc: 0.6250, Test Acc: 0.5850, Train Loss: 0.6662, Test Loss: 0.6797\n",
      "Epoch: 530, Train Acc: 0.6185, Test Acc: 0.5350, Train Loss: 0.6658, Test Loss: 0.6791\n",
      "Epoch: 540, Train Acc: 0.6099, Test Acc: 0.5400, Train Loss: 0.6645, Test Loss: 0.6787\n",
      "Epoch: 550, Train Acc: 0.6336, Test Acc: 0.6000, Train Loss: 0.6629, Test Loss: 0.6785\n",
      "Epoch: 560, Train Acc: 0.6250, Test Acc: 0.5800, Train Loss: 0.6620, Test Loss: 0.6780\n",
      "Epoch: 570, Train Acc: 0.6358, Test Acc: 0.6050, Train Loss: 0.6593, Test Loss: 0.6777\n",
      "Epoch: 580, Train Acc: 0.6401, Test Acc: 0.6050, Train Loss: 0.6611, Test Loss: 0.6775\n",
      "Epoch: 590, Train Acc: 0.6185, Test Acc: 0.5550, Train Loss: 0.6590, Test Loss: 0.6772\n",
      "Epoch: 600, Train Acc: 0.6250, Test Acc: 0.5500, Train Loss: 0.6583, Test Loss: 0.6767\n",
      "Epoch: 610, Train Acc: 0.6293, Test Acc: 0.6150, Train Loss: 0.6571, Test Loss: 0.6766\n",
      "Epoch: 620, Train Acc: 0.6444, Test Acc: 0.5650, Train Loss: 0.6567, Test Loss: 0.6758\n",
      "Epoch: 630, Train Acc: 0.6401, Test Acc: 0.5950, Train Loss: 0.6542, Test Loss: 0.6755\n",
      "Epoch: 640, Train Acc: 0.6379, Test Acc: 0.5650, Train Loss: 0.6544, Test Loss: 0.6754\n",
      "Epoch: 650, Train Acc: 0.6487, Test Acc: 0.6000, Train Loss: 0.6521, Test Loss: 0.6756\n",
      "Epoch: 660, Train Acc: 0.6466, Test Acc: 0.5650, Train Loss: 0.6512, Test Loss: 0.6745\n",
      "Epoch: 670, Train Acc: 0.6573, Test Acc: 0.6150, Train Loss: 0.6486, Test Loss: 0.6746\n",
      "Epoch: 680, Train Acc: 0.6466, Test Acc: 0.5650, Train Loss: 0.6500, Test Loss: 0.6741\n",
      "Epoch: 690, Train Acc: 0.6487, Test Acc: 0.5700, Train Loss: 0.6486, Test Loss: 0.6738\n",
      "Epoch: 700, Train Acc: 0.6466, Test Acc: 0.6100, Train Loss: 0.6460, Test Loss: 0.6739\n",
      "Epoch: 710, Train Acc: 0.6530, Test Acc: 0.5750, Train Loss: 0.6446, Test Loss: 0.6729\n",
      "Epoch: 720, Train Acc: 0.6573, Test Acc: 0.5950, Train Loss: 0.6438, Test Loss: 0.6722\n",
      "Epoch: 730, Train Acc: 0.6681, Test Acc: 0.6000, Train Loss: 0.6406, Test Loss: 0.6719\n",
      "Epoch: 740, Train Acc: 0.6358, Test Acc: 0.5750, Train Loss: 0.6425, Test Loss: 0.6730\n",
      "Epoch: 750, Train Acc: 0.6746, Test Acc: 0.6150, Train Loss: 0.6375, Test Loss: 0.6712\n",
      "Epoch: 760, Train Acc: 0.6681, Test Acc: 0.6250, Train Loss: 0.6371, Test Loss: 0.6713\n",
      "Epoch: 770, Train Acc: 0.6681, Test Acc: 0.6200, Train Loss: 0.6356, Test Loss: 0.6708\n",
      "Epoch: 780, Train Acc: 0.6746, Test Acc: 0.6050, Train Loss: 0.6344, Test Loss: 0.6699\n",
      "Epoch: 790, Train Acc: 0.6746, Test Acc: 0.6050, Train Loss: 0.6339, Test Loss: 0.6695\n",
      "Epoch: 800, Train Acc: 0.6897, Test Acc: 0.6150, Train Loss: 0.6306, Test Loss: 0.6691\n",
      "Epoch: 810, Train Acc: 0.6789, Test Acc: 0.6050, Train Loss: 0.6274, Test Loss: 0.6685\n",
      "Epoch: 820, Train Acc: 0.6767, Test Acc: 0.6150, Train Loss: 0.6269, Test Loss: 0.6682\n",
      "Epoch: 830, Train Acc: 0.6789, Test Acc: 0.6150, Train Loss: 0.6245, Test Loss: 0.6682\n",
      "Epoch: 840, Train Acc: 0.6875, Test Acc: 0.6150, Train Loss: 0.6243, Test Loss: 0.6677\n",
      "Epoch: 850, Train Acc: 0.6853, Test Acc: 0.5950, Train Loss: 0.6216, Test Loss: 0.6670\n",
      "Epoch: 860, Train Acc: 0.6961, Test Acc: 0.6250, Train Loss: 0.6190, Test Loss: 0.6677\n",
      "Epoch: 870, Train Acc: 0.6810, Test Acc: 0.5700, Train Loss: 0.6237, Test Loss: 0.6697\n",
      "Epoch: 880, Train Acc: 0.7026, Test Acc: 0.6200, Train Loss: 0.6151, Test Loss: 0.6671\n",
      "Epoch: 890, Train Acc: 0.7112, Test Acc: 0.6300, Train Loss: 0.6125, Test Loss: 0.6659\n",
      "Epoch: 900, Train Acc: 0.7177, Test Acc: 0.6150, Train Loss: 0.6109, Test Loss: 0.6650\n",
      "Epoch: 910, Train Acc: 0.7091, Test Acc: 0.6100, Train Loss: 0.6096, Test Loss: 0.6648\n",
      "Epoch: 920, Train Acc: 0.7198, Test Acc: 0.6200, Train Loss: 0.6082, Test Loss: 0.6645\n",
      "Epoch: 930, Train Acc: 0.7134, Test Acc: 0.6100, Train Loss: 0.6063, Test Loss: 0.6643\n",
      "Epoch: 940, Train Acc: 0.7134, Test Acc: 0.6100, Train Loss: 0.6023, Test Loss: 0.6639\n",
      "Epoch: 950, Train Acc: 0.7198, Test Acc: 0.6200, Train Loss: 0.6011, Test Loss: 0.6645\n",
      "Epoch: 960, Train Acc: 0.7284, Test Acc: 0.6200, Train Loss: 0.5992, Test Loss: 0.6631\n",
      "Epoch: 970, Train Acc: 0.7220, Test Acc: 0.6150, Train Loss: 0.5938, Test Loss: 0.6627\n",
      "Epoch: 980, Train Acc: 0.7241, Test Acc: 0.6150, Train Loss: 0.5948, Test Loss: 0.6626\n",
      "Epoch: 990, Train Acc: 0.7069, Test Acc: 0.5950, Train Loss: 0.5966, Test Loss: 0.6658\n",
      "Epoch: 1000, Train Acc: 0.7349, Test Acc: 0.6300, Train Loss: 0.5873, Test Loss: 0.6619\n",
      "Epoch: 1010, Train Acc: 0.7220, Test Acc: 0.6150, Train Loss: 0.5879, Test Loss: 0.6652\n",
      "Epoch: 1020, Train Acc: 0.7392, Test Acc: 0.6300, Train Loss: 0.5843, Test Loss: 0.6626\n",
      "Epoch: 1030, Train Acc: 0.7392, Test Acc: 0.6200, Train Loss: 0.5819, Test Loss: 0.6642\n",
      "Epoch: 1040, Train Acc: 0.7414, Test Acc: 0.6300, Train Loss: 0.5788, Test Loss: 0.6637\n",
      "Epoch: 1050, Train Acc: 0.7414, Test Acc: 0.6200, Train Loss: 0.5749, Test Loss: 0.6618\n",
      "Epoch: 1060, Train Acc: 0.7543, Test Acc: 0.6400, Train Loss: 0.5732, Test Loss: 0.6608\n",
      "Epoch: 1070, Train Acc: 0.7543, Test Acc: 0.6300, Train Loss: 0.5707, Test Loss: 0.6599\n",
      "Epoch: 1080, Train Acc: 0.7586, Test Acc: 0.6300, Train Loss: 0.5695, Test Loss: 0.6596\n",
      "Epoch: 1090, Train Acc: 0.7414, Test Acc: 0.6250, Train Loss: 0.5706, Test Loss: 0.6608\n",
      "Epoch: 1100, Train Acc: 0.7672, Test Acc: 0.6300, Train Loss: 0.5647, Test Loss: 0.6597\n",
      "Epoch: 1110, Train Acc: 0.7629, Test Acc: 0.6250, Train Loss: 0.5622, Test Loss: 0.6616\n",
      "Epoch: 1120, Train Acc: 0.7651, Test Acc: 0.6350, Train Loss: 0.5590, Test Loss: 0.6632\n",
      "Epoch: 1130, Train Acc: 0.7478, Test Acc: 0.6250, Train Loss: 0.5596, Test Loss: 0.6606\n",
      "Epoch: 1140, Train Acc: 0.7457, Test Acc: 0.6250, Train Loss: 0.5570, Test Loss: 0.6615\n",
      "Epoch: 1150, Train Acc: 0.7737, Test Acc: 0.6300, Train Loss: 0.5510, Test Loss: 0.6597\n",
      "Epoch: 1160, Train Acc: 0.7694, Test Acc: 0.6500, Train Loss: 0.5486, Test Loss: 0.6596\n",
      "Epoch: 1170, Train Acc: 0.7823, Test Acc: 0.6400, Train Loss: 0.5450, Test Loss: 0.6624\n",
      "Epoch: 1180, Train Acc: 0.7586, Test Acc: 0.6150, Train Loss: 0.5478, Test Loss: 0.6680\n",
      "Epoch: 1190, Train Acc: 0.7694, Test Acc: 0.6450, Train Loss: 0.5416, Test Loss: 0.6592\n",
      "Epoch: 1200, Train Acc: 0.6875, Test Acc: 0.5800, Train Loss: 0.5711, Test Loss: 0.6812\n",
      "Epoch: 1210, Train Acc: 0.7629, Test Acc: 0.6400, Train Loss: 0.5334, Test Loss: 0.6592\n",
      "Epoch: 1220, Train Acc: 0.7866, Test Acc: 0.6400, Train Loss: 0.5329, Test Loss: 0.6618\n",
      "Epoch: 1230, Train Acc: 0.7845, Test Acc: 0.6400, Train Loss: 0.5289, Test Loss: 0.6618\n",
      "Epoch: 1240, Train Acc: 0.7672, Test Acc: 0.6400, Train Loss: 0.5289, Test Loss: 0.6599\n",
      "Epoch: 1250, Train Acc: 0.7802, Test Acc: 0.6450, Train Loss: 0.5267, Test Loss: 0.6602\n",
      "Epoch: 1260, Train Acc: 0.6315, Test Acc: 0.5400, Train Loss: 0.5782, Test Loss: 0.7275\n",
      "Epoch: 1270, Train Acc: 0.7802, Test Acc: 0.6200, Train Loss: 0.5222, Test Loss: 0.6674\n",
      "Epoch: 1280, Train Acc: 0.7759, Test Acc: 0.6500, Train Loss: 0.5180, Test Loss: 0.6598\n",
      "Epoch: 1290, Train Acc: 0.7241, Test Acc: 0.5850, Train Loss: 0.5318, Test Loss: 0.6840\n",
      "Epoch: 1300, Train Acc: 0.7737, Test Acc: 0.6150, Train Loss: 0.5258, Test Loss: 0.6647\n",
      "Epoch: 1310, Train Acc: 0.7737, Test Acc: 0.6450, Train Loss: 0.5161, Test Loss: 0.6630\n",
      "Epoch: 1320, Train Acc: 0.7845, Test Acc: 0.6350, Train Loss: 0.5117, Test Loss: 0.6616\n",
      "Epoch: 1330, Train Acc: 0.7543, Test Acc: 0.6050, Train Loss: 0.5231, Test Loss: 0.6713\n",
      "Epoch: 1340, Train Acc: 0.7909, Test Acc: 0.6300, Train Loss: 0.5031, Test Loss: 0.6683\n",
      "Epoch: 1350, Train Acc: 0.7909, Test Acc: 0.6100, Train Loss: 0.5031, Test Loss: 0.6753\n",
      "Epoch: 1360, Train Acc: 0.7974, Test Acc: 0.6400, Train Loss: 0.4968, Test Loss: 0.6658\n",
      "Epoch: 1370, Train Acc: 0.7909, Test Acc: 0.6350, Train Loss: 0.4977, Test Loss: 0.6639\n",
      "Epoch: 1380, Train Acc: 0.7759, Test Acc: 0.6250, Train Loss: 0.5045, Test Loss: 0.6679\n",
      "Epoch: 1390, Train Acc: 0.7909, Test Acc: 0.6350, Train Loss: 0.4935, Test Loss: 0.6651\n",
      "Epoch: 1400, Train Acc: 0.7996, Test Acc: 0.6400, Train Loss: 0.4857, Test Loss: 0.6659\n",
      "Epoch: 1410, Train Acc: 0.7974, Test Acc: 0.6200, Train Loss: 0.4860, Test Loss: 0.6719\n",
      "Epoch: 1420, Train Acc: 0.7780, Test Acc: 0.6200, Train Loss: 0.4977, Test Loss: 0.6721\n",
      "Epoch: 1430, Train Acc: 0.7241, Test Acc: 0.5800, Train Loss: 0.5067, Test Loss: 0.7038\n",
      "Epoch: 1440, Train Acc: 0.7435, Test Acc: 0.6050, Train Loss: 0.5085, Test Loss: 0.6868\n",
      "Epoch: 1450, Train Acc: 0.7974, Test Acc: 0.6550, Train Loss: 0.4772, Test Loss: 0.6673\n",
      "Epoch: 1460, Train Acc: 0.8060, Test Acc: 0.6500, Train Loss: 0.4746, Test Loss: 0.6681\n",
      "Epoch: 1470, Train Acc: 0.8017, Test Acc: 0.6100, Train Loss: 0.4767, Test Loss: 0.6824\n",
      "Epoch: 1480, Train Acc: 0.8082, Test Acc: 0.6300, Train Loss: 0.4690, Test Loss: 0.6747\n",
      "Epoch: 1490, Train Acc: 0.7629, Test Acc: 0.5800, Train Loss: 0.4855, Test Loss: 0.7034\n",
      "Epoch: 1500, Train Acc: 0.8060, Test Acc: 0.6600, Train Loss: 0.4631, Test Loss: 0.6710\n",
      "Epoch: 1510, Train Acc: 0.8103, Test Acc: 0.6100, Train Loss: 0.4670, Test Loss: 0.6851\n",
      "Epoch: 1520, Train Acc: 0.7134, Test Acc: 0.5950, Train Loss: 0.4965, Test Loss: 0.7240\n",
      "Epoch: 1530, Train Acc: 0.7543, Test Acc: 0.6000, Train Loss: 0.4953, Test Loss: 0.6947\n",
      "Epoch: 1540, Train Acc: 0.7629, Test Acc: 0.6050, Train Loss: 0.4769, Test Loss: 0.7087\n",
      "Epoch: 1550, Train Acc: 0.8211, Test Acc: 0.6350, Train Loss: 0.4525, Test Loss: 0.6766\n",
      "Epoch: 1560, Train Acc: 0.8147, Test Acc: 0.6350, Train Loss: 0.4540, Test Loss: 0.6751\n",
      "Epoch: 1570, Train Acc: 0.8254, Test Acc: 0.6250, Train Loss: 0.4470, Test Loss: 0.6790\n",
      "Epoch: 1580, Train Acc: 0.7909, Test Acc: 0.6300, Train Loss: 0.4622, Test Loss: 0.6845\n",
      "Epoch: 1590, Train Acc: 0.8190, Test Acc: 0.6550, Train Loss: 0.4447, Test Loss: 0.6773\n",
      "Epoch: 1600, Train Acc: 0.8233, Test Acc: 0.6450, Train Loss: 0.4419, Test Loss: 0.6787\n",
      "Epoch: 1610, Train Acc: 0.8211, Test Acc: 0.6500, Train Loss: 0.4395, Test Loss: 0.6792\n",
      "Epoch: 1620, Train Acc: 0.8254, Test Acc: 0.6350, Train Loss: 0.4367, Test Loss: 0.6814\n",
      "Epoch: 1630, Train Acc: 0.8168, Test Acc: 0.6250, Train Loss: 0.4449, Test Loss: 0.7021\n",
      "Epoch: 1640, Train Acc: 0.8211, Test Acc: 0.6350, Train Loss: 0.4426, Test Loss: 0.6855\n",
      "Epoch: 1650, Train Acc: 0.8297, Test Acc: 0.6500, Train Loss: 0.4330, Test Loss: 0.6825\n",
      "Epoch: 1660, Train Acc: 0.8168, Test Acc: 0.6100, Train Loss: 0.4376, Test Loss: 0.7028\n",
      "Epoch: 1670, Train Acc: 0.8297, Test Acc: 0.6400, Train Loss: 0.4303, Test Loss: 0.6850\n",
      "Epoch: 1680, Train Acc: 0.8384, Test Acc: 0.6500, Train Loss: 0.4224, Test Loss: 0.6867\n",
      "Epoch: 1690, Train Acc: 0.8103, Test Acc: 0.6200, Train Loss: 0.4410, Test Loss: 0.7144\n",
      "Epoch: 1700, Train Acc: 0.8362, Test Acc: 0.6500, Train Loss: 0.4211, Test Loss: 0.6860\n",
      "Epoch: 1710, Train Acc: 0.8362, Test Acc: 0.6400, Train Loss: 0.4193, Test Loss: 0.6882\n",
      "Epoch: 1720, Train Acc: 0.8297, Test Acc: 0.6100, Train Loss: 0.4208, Test Loss: 0.7038\n",
      "Epoch: 1730, Train Acc: 0.7565, Test Acc: 0.6000, Train Loss: 0.4546, Test Loss: 0.7486\n",
      "Epoch: 1740, Train Acc: 0.8276, Test Acc: 0.6450, Train Loss: 0.4209, Test Loss: 0.6946\n",
      "Epoch: 1750, Train Acc: 0.7155, Test Acc: 0.5550, Train Loss: 0.4861, Test Loss: 0.7916\n",
      "Epoch: 1760, Train Acc: 0.8125, Test Acc: 0.6400, Train Loss: 0.4261, Test Loss: 0.7007\n",
      "Epoch: 1770, Train Acc: 0.8125, Test Acc: 0.6300, Train Loss: 0.4225, Test Loss: 0.7236\n",
      "Epoch: 1780, Train Acc: 0.8427, Test Acc: 0.6450, Train Loss: 0.4071, Test Loss: 0.6937\n",
      "Epoch: 1790, Train Acc: 0.8427, Test Acc: 0.6500, Train Loss: 0.4005, Test Loss: 0.6963\n",
      "Epoch: 1800, Train Acc: 0.8211, Test Acc: 0.6200, Train Loss: 0.4104, Test Loss: 0.7167\n",
      "Epoch: 1810, Train Acc: 0.8297, Test Acc: 0.6200, Train Loss: 0.4077, Test Loss: 0.7144\n",
      "Epoch: 1820, Train Acc: 0.8405, Test Acc: 0.6450, Train Loss: 0.4067, Test Loss: 0.6992\n",
      "Epoch: 1830, Train Acc: 0.7155, Test Acc: 0.5400, Train Loss: 0.4835, Test Loss: 0.8138\n",
      "Epoch: 1840, Train Acc: 0.8405, Test Acc: 0.6450, Train Loss: 0.4015, Test Loss: 0.7021\n",
      "Epoch: 1850, Train Acc: 0.8297, Test Acc: 0.6150, Train Loss: 0.4009, Test Loss: 0.7220\n",
      "Epoch: 1860, Train Acc: 0.7716, Test Acc: 0.6050, Train Loss: 0.4308, Test Loss: 0.7649\n",
      "Epoch: 1870, Train Acc: 0.8254, Test Acc: 0.6400, Train Loss: 0.4090, Test Loss: 0.7122\n",
      "Epoch: 1880, Train Acc: 0.8556, Test Acc: 0.6350, Train Loss: 0.3850, Test Loss: 0.7094\n",
      "Epoch: 1890, Train Acc: 0.8578, Test Acc: 0.6550, Train Loss: 0.3875, Test Loss: 0.7061\n",
      "Epoch: 1900, Train Acc: 0.8017, Test Acc: 0.6200, Train Loss: 0.4145, Test Loss: 0.7532\n",
      "Epoch: 1910, Train Acc: 0.8125, Test Acc: 0.6150, Train Loss: 0.4008, Test Loss: 0.7415\n",
      "Epoch: 1920, Train Acc: 0.8534, Test Acc: 0.6200, Train Loss: 0.3819, Test Loss: 0.7156\n",
      "Epoch: 1930, Train Acc: 0.8297, Test Acc: 0.6200, Train Loss: 0.3899, Test Loss: 0.7353\n",
      "Epoch: 1940, Train Acc: 0.8513, Test Acc: 0.6250, Train Loss: 0.3799, Test Loss: 0.7249\n",
      "Epoch: 1950, Train Acc: 0.8578, Test Acc: 0.6450, Train Loss: 0.3752, Test Loss: 0.7124\n",
      "Epoch: 1960, Train Acc: 0.8491, Test Acc: 0.6250, Train Loss: 0.3769, Test Loss: 0.7276\n",
      "Epoch: 1970, Train Acc: 0.7565, Test Acc: 0.5600, Train Loss: 0.4359, Test Loss: 0.8085\n",
      "Epoch: 1980, Train Acc: 0.8534, Test Acc: 0.6450, Train Loss: 0.3713, Test Loss: 0.7157\n",
      "Epoch: 1990, Train Acc: 0.8707, Test Acc: 0.6350, Train Loss: 0.3642, Test Loss: 0.7228\n",
      "Epoch: 2000, Train Acc: 0.8513, Test Acc: 0.6450, Train Loss: 0.3667, Test Loss: 0.7183\n",
      "Epoch: 2010, Train Acc: 0.8362, Test Acc: 0.6350, Train Loss: 0.3769, Test Loss: 0.7250\n",
      "Epoch: 2020, Train Acc: 0.8534, Test Acc: 0.6500, Train Loss: 0.3671, Test Loss: 0.7210\n",
      "Epoch: 2030, Train Acc: 0.8664, Test Acc: 0.6300, Train Loss: 0.3645, Test Loss: 0.7315\n",
      "Epoch: 2040, Train Acc: 0.8384, Test Acc: 0.6450, Train Loss: 0.3777, Test Loss: 0.7315\n",
      "Epoch: 2050, Train Acc: 0.8125, Test Acc: 0.6050, Train Loss: 0.3874, Test Loss: 0.7747\n",
      "Epoch: 2060, Train Acc: 0.7953, Test Acc: 0.6150, Train Loss: 0.3915, Test Loss: 0.7852\n",
      "Epoch: 2070, Train Acc: 0.8362, Test Acc: 0.6150, Train Loss: 0.3678, Test Loss: 0.7537\n",
      "Epoch: 2080, Train Acc: 0.8685, Test Acc: 0.6450, Train Loss: 0.3514, Test Loss: 0.7288\n",
      "Epoch: 2090, Train Acc: 0.8685, Test Acc: 0.6450, Train Loss: 0.3498, Test Loss: 0.7302\n",
      "Epoch: 2100, Train Acc: 0.8513, Test Acc: 0.6400, Train Loss: 0.3592, Test Loss: 0.7336\n",
      "Epoch: 2110, Train Acc: 0.8772, Test Acc: 0.6400, Train Loss: 0.3468, Test Loss: 0.7315\n",
      "Epoch: 2120, Train Acc: 0.7909, Test Acc: 0.6000, Train Loss: 0.3925, Test Loss: 0.8023\n",
      "Epoch: 2130, Train Acc: 0.8707, Test Acc: 0.6450, Train Loss: 0.3463, Test Loss: 0.7345\n",
      "Epoch: 2140, Train Acc: 0.8534, Test Acc: 0.6500, Train Loss: 0.3480, Test Loss: 0.7350\n",
      "Epoch: 2150, Train Acc: 0.8233, Test Acc: 0.6150, Train Loss: 0.3696, Test Loss: 0.7818\n",
      "Epoch: 2160, Train Acc: 0.8556, Test Acc: 0.6400, Train Loss: 0.3508, Test Loss: 0.7395\n",
      "Epoch: 2170, Train Acc: 0.7953, Test Acc: 0.5900, Train Loss: 0.3799, Test Loss: 0.8017\n",
      "Epoch: 2180, Train Acc: 0.8793, Test Acc: 0.6400, Train Loss: 0.3366, Test Loss: 0.7402\n",
      "Epoch: 2190, Train Acc: 0.8405, Test Acc: 0.6450, Train Loss: 0.3660, Test Loss: 0.7580\n",
      "Epoch: 2200, Train Acc: 0.8599, Test Acc: 0.6200, Train Loss: 0.3461, Test Loss: 0.7682\n",
      "Epoch: 2210, Train Acc: 0.7888, Test Acc: 0.5750, Train Loss: 0.3950, Test Loss: 0.8328\n",
      "Epoch: 2220, Train Acc: 0.8685, Test Acc: 0.6250, Train Loss: 0.3366, Test Loss: 0.7630\n",
      "Epoch: 2230, Train Acc: 0.7823, Test Acc: 0.5850, Train Loss: 0.3940, Test Loss: 0.8395\n",
      "Epoch: 2240, Train Acc: 0.8664, Test Acc: 0.6500, Train Loss: 0.3360, Test Loss: 0.7518\n",
      "Epoch: 2250, Train Acc: 0.8556, Test Acc: 0.6200, Train Loss: 0.3375, Test Loss: 0.7740\n",
      "Epoch: 2260, Train Acc: 0.8664, Test Acc: 0.6550, Train Loss: 0.3335, Test Loss: 0.7542\n",
      "Epoch: 2270, Train Acc: 0.8922, Test Acc: 0.6350, Train Loss: 0.3223, Test Loss: 0.7574\n",
      "Epoch: 2280, Train Acc: 0.8858, Test Acc: 0.6450, Train Loss: 0.3230, Test Loss: 0.7528\n",
      "Epoch: 2290, Train Acc: 0.8836, Test Acc: 0.6450, Train Loss: 0.3194, Test Loss: 0.7544\n",
      "Epoch: 2300, Train Acc: 0.8750, Test Acc: 0.6250, Train Loss: 0.3286, Test Loss: 0.7764\n",
      "Epoch: 2310, Train Acc: 0.8341, Test Acc: 0.5950, Train Loss: 0.3524, Test Loss: 0.8156\n",
      "Epoch: 2320, Train Acc: 0.8685, Test Acc: 0.6600, Train Loss: 0.3307, Test Loss: 0.7664\n",
      "Epoch: 2330, Train Acc: 0.8578, Test Acc: 0.6450, Train Loss: 0.3396, Test Loss: 0.7711\n",
      "Epoch: 2340, Train Acc: 0.8534, Test Acc: 0.6250, Train Loss: 0.3295, Test Loss: 0.7914\n",
      "Epoch: 2350, Train Acc: 0.8772, Test Acc: 0.6200, Train Loss: 0.3156, Test Loss: 0.7803\n",
      "Epoch: 2360, Train Acc: 0.8879, Test Acc: 0.6500, Train Loss: 0.3093, Test Loss: 0.7643\n",
      "Epoch: 2370, Train Acc: 0.8685, Test Acc: 0.6100, Train Loss: 0.3226, Test Loss: 0.7943\n",
      "Epoch: 2380, Train Acc: 0.9030, Test Acc: 0.6500, Train Loss: 0.3058, Test Loss: 0.7712\n",
      "Epoch: 2390, Train Acc: 0.8728, Test Acc: 0.6500, Train Loss: 0.3189, Test Loss: 0.7737\n",
      "Epoch: 2400, Train Acc: 0.8685, Test Acc: 0.6500, Train Loss: 0.3193, Test Loss: 0.7754\n",
      "Epoch: 2410, Train Acc: 0.9009, Test Acc: 0.6400, Train Loss: 0.3048, Test Loss: 0.7730\n",
      "Epoch: 2420, Train Acc: 0.7802, Test Acc: 0.5700, Train Loss: 0.3911, Test Loss: 0.8919\n",
      "Epoch: 2430, Train Acc: 0.8944, Test Acc: 0.6200, Train Loss: 0.3034, Test Loss: 0.7865\n",
      "Epoch: 2440, Train Acc: 0.8578, Test Acc: 0.6150, Train Loss: 0.3204, Test Loss: 0.8124\n",
      "Epoch: 2450, Train Acc: 0.8341, Test Acc: 0.6000, Train Loss: 0.3441, Test Loss: 0.8435\n",
      "Epoch: 2460, Train Acc: 0.8901, Test Acc: 0.6150, Train Loss: 0.3050, Test Loss: 0.7978\n",
      "Epoch: 2470, Train Acc: 0.8082, Test Acc: 0.5750, Train Loss: 0.3560, Test Loss: 0.8650\n",
      "Epoch: 2480, Train Acc: 0.8922, Test Acc: 0.6200, Train Loss: 0.2959, Test Loss: 0.7964\n",
      "Epoch: 2490, Train Acc: 0.7931, Test Acc: 0.5700, Train Loss: 0.3700, Test Loss: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>▁▁▂▁▁▁▂▃▄▄▅▄▆▆▆▇▇▄▇▇▇█▆▇▆▇▇▇▆█▆▆▇▇▇█▃███</td></tr><tr><td>Test F1</td><td>▇███▇▇█▄▅▄▄▁▅▄▇▅▅▇▇▇▇▇▇▄▃▇█▇█▆▇█▆▇▇▇▇▇▇▇</td></tr><tr><td>Test Loss</td><td>▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▂▂▂▂▃▂▃▂▃▅▃▅▄▄█▄▅▅</td></tr><tr><td>Test Sensitivity</td><td>▇███▇▇█▃▄▃▃▁▃▃▅▃▃▆▄▄▅▄▅▂▁▄▅▄▅▃▅▅▄▅▄▄▆▄▄▄</td></tr><tr><td>Test Specificity</td><td>▁▁▁▁▂▂▂▅▅▆▇▇▇▇▅▇▇▃▆▆▆▇▅██▆▅▆▅▇▆▅▆▅▆▇▃▇▇▇</td></tr><tr><td>Train Accuracy</td><td>▁▁▁▁▁▁▂▂▃▃▃▃▄▄▄▅▅▄▅▆▆▆▆▆▅▇▆▇▆▇▇▆▇▇██▆███</td></tr><tr><td>Train F1</td><td>▃▃▄▄▃▃▄▂▃▃▃▁▃▃▅▄▄▅▅▆▆▆▆▅▄▇▆▇▆▇▇▆▇▇█▇▆███</td></tr><tr><td>Train Loss</td><td>█████████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▅▄▄▃▃▃▃▃▂▂▂▂▃▁▁▁</td></tr><tr><td>Train Sensitivity</td><td>▇███▆▆▇▄▄▃▃▁▃▃▅▄▃▇▅▅▆▅▆▃▂▆▇▆▇▅▆▇▆▇▆▆█▆▆▆</td></tr><tr><td>Train Specificity</td><td>▂▁▁▁▂▂▂▅▅▅▆▇▆▆▅▇▇▄▇▇▆▇▆██▇▆▇▆█▇▆█▆▇█▅███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Accuracy</td><td>0.655</td></tr><tr><td>Test F1</td><td>0.64615</td></tr><tr><td>Test Loss</td><td>0.78587</td></tr><tr><td>Test Sensitivity</td><td>0.61165</td></tr><tr><td>Test Specificity</td><td>0.70103</td></tr><tr><td>Train Accuracy</td><td>0.88578</td></tr><tr><td>Train F1</td><td>0.88403</td></tr><tr><td>Train Loss</td><td>0.29923</td></tr><tr><td>Train Sensitivity</td><td>0.82114</td></tr><tr><td>Train Specificity</td><td>0.95872</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Only_SBMLCore_9-2500epochs</strong> at: <a href='https://wandb.ai/dylan-home/train-final-model/runs/eh4pnwna' target=\"_blank\">https://wandb.ai/dylan-home/train-final-model/runs/eh4pnwna</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250714_103342-eh4pnwna/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run_name = f'Run 7 - {epochs} epochs - lr {learning_rate}'\n",
    "run_name = f'Only_SBMLCore_9-{epochs}epochs'\n",
    "\n",
    "model = run_model.pnca_GCN_vary_graph(\n",
    "            self_loops = False,\n",
    "            cutoff_distance = cutoff_distance,\n",
    "            edge_weight_func = edge_weight_func,\n",
    "            batch_size = batch_size,\n",
    "            num_node_features = num_node_features,\n",
    "            hidden_channels = hidden_channels,\n",
    "            learning_rate = learning_rate,\n",
    "            wd = wd,\n",
    "            dropout = dropout,\n",
    "            lr_scheduling=False,\n",
    "            epochs = epochs,\n",
    "            graph_dict= graph_dict,\n",
    "            normalise_ews=True,\n",
    "            lambda_param= edge_weight_lambda,\n",
    "            early_stop=False,\n",
    "            no_node_mpfs=True,\n",
    "            save_path= f'saved_models/{project}/{run_name}',\n",
    "            wandb_params={\n",
    "              'use_wandb': True, \n",
    "              'wandb_project': f'{project}', \n",
    "              'wandb_name': f'{run_name}',\n",
    "              'n_samples': n_samples,\n",
    "              'sweep': False\n",
    "              }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[185, 18], edge_index=[2, 5452], edge_attr=[5452, 1], y=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_dict['train']['pnca_mut_0']['graph'].dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
